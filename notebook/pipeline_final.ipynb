{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import ast\n",
    "from event_loop.preprocessing.dataframe import *\n",
    "\n",
    "import pandas as pd\n",
    "from const import *\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:04.277593Z",
     "start_time": "2024-05-07T08:37:04.275797Z"
    }
   },
   "id": "e78c62da13f69969"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "load data and determine parameter settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71fca8f257d622"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "SETTING = \"HR\"  # Change to \"PTP\" for Purchase To Pay Stream Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:04.281609Z",
     "start_time": "2024-05-07T08:37:04.278527Z"
    }
   },
   "id": "cac378f56875dc75"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Parse json-like Message Attributes Column as object\n",
    "converters={\"MessageAttributes\": ast.literal_eval}\n",
    "\n",
    "if SETTING == \"HR\": \n",
    "    # path to training data file\n",
    "    train_path = HR_TRAIN_PATH\n",
    "    # path to interleaved data file for streaming\n",
    "    il_path = HR_IL_PATH\n",
    "    # path to ground truth data\n",
    "    gt_path = HR_GT_PATH\n",
    "    # path to BPMN reference model\n",
    "    bpmn_path = HR_BPMN_PATH\n",
    "    # data attributes for the HR process\n",
    "    attributes = HR_ATTRIBUTES\n",
    "    # case ID indicator attribute\n",
    "    case_id_primary = HR_CASE_ID_PRIMARY\n",
    "    \n",
    "    \n",
    "elif SETTING == \"PTP\": \n",
    "    train_path = PTP_TRAIN_PATH\n",
    "    il_path = PTP_IL_PATH\n",
    "    gt_path = PTP_GT_PATH\n",
    "    bpmn_path = PTP_BPMN_PATH\n",
    "    attributes = PTP_ATTRIBUTES\n",
    "    case_id_primary = PTP_CASE_ID_PRIMARY"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:04.285563Z",
     "start_time": "2024-05-07T08:37:04.282208Z"
    }
   },
   "id": "49b3846d32a49807"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train_in = pd.read_csv(train_path, converters=converters)\n",
    "df_il_in = pd.read_csv(il_path, converters=converters)\n",
    "df_gt = pd.read_csv(gt_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:06.793180Z",
     "start_time": "2024-05-07T08:37:04.285674Z"
    }
   },
   "id": "888f7772868d3c7e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Warm Up Phase\n",
    "preprocess training data and fit models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cdfd16523df0b5b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# preprocess training data\n",
    "df_train = pre_process(df_train_in)\n",
    "df_train = assign_sequence_number(df_train)\n",
    "df_train = mark_start_end(df_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:16.558106Z",
     "start_time": "2024-05-07T08:37:06.800473Z"
    }
   },
   "id": "195cf763c2fc9f23"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# according to Hadad et al. trace #128 contains erroneous data - exclude it (for PTP)\n",
    "df_train_flt = df_train[~df_train[\"SequenceNumber\"].isin([128])]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:16.566635Z",
     "start_time": "2024-05-07T08:37:16.559070Z"
    }
   },
   "id": "ef9a7078a4d6971f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from event_loop.event_activity import EventActivityAssignment\n",
    "from event_loop.activity_type import ActivityTypeClassifier\n",
    "from event_loop.activity_boundaries import ActivityBoundariesClassifier\n",
    "\n",
    "# initialize and fit models\n",
    "\n",
    "activity_boundaries_classifier = ActivityBoundariesClassifier(df_train_flt, None)\n",
    "\n",
    "activity_type_classifier = ActivityTypeClassifier(df_train)\n",
    "\n",
    "event_activity_model = EventActivityAssignment(df_train,10, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:32.392289Z",
     "start_time": "2024-05-07T08:37:16.567439Z"
    }
   },
   "id": "5c0de69be52bb6ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Streaming Phase\n",
    "\n",
    "Main loop. Gets raw R1 data as input. \n",
    "Applies filtering, activity action and sequence classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd4922a5f25b0f9d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "records = df_il_in.to_dict(\"records\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:32.663159Z",
     "start_time": "2024-05-07T08:37:32.446950Z"
    }
   },
   "id": "e79d8e1cbb43d45d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Event Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c351bfa00c023802"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from event_loop.event import Event\n",
    "from event_loop.stack import Stack\n",
    "from event_loop.event_activity import search_stack_for_request_frame, search_stream_index\n",
    "\n",
    "import time\n",
    "from event_loop.preprocessing.event import keep_event\n",
    "\n",
    "# Parameters for the Event Loop\n",
    "EVENT_LOOP_CUTOFF_NO_ACTION = 3\n",
    "EVENT_LOOP_CUTOFF_END_EVENT = 3\n",
    "ENTROPY_THRESHOLD = 0.4 #0.5\n",
    "MAX_WINDOW_SIZE = 10\n",
    "VERBOSE = False\n",
    "\n",
    "event_buffer: list[Event] = []\n",
    "attribute_buffer: list[dict] = []\n",
    "stacks: list[Stack] = []\n",
    "stacks_out: list[Stack] = []\n",
    "event_loop_index = 0\n",
    "\n",
    "# variables for logging processing times and buffer sizes\n",
    "processing_times = []\n",
    "processing_times_filter = []\n",
    "buffer_sizes = []\n",
    "\n",
    "# start of event loop\n",
    "for i, event_data in enumerate(records):\n",
    "    start_time = time.time()\n",
    "\n",
    "    buffer_sizes.append(sum([len(stack) for stack in stacks]))\n",
    "    # filter raw event stream\n",
    "    if not keep_event(event_data):\n",
    "        end_time = time.time()\n",
    "        processing_times_filter.append(end_time - start_time)\n",
    "        # skip event in loop\n",
    "        continue\n",
    "    \n",
    "    # count every kept event for event loop index\n",
    "    event_loop_index += 1\n",
    "\n",
    "    # Generate Network Event from Raw Network Package\n",
    "    event = Event(event_data, event_loop_index, event_buffer, SETTING)\n",
    "    event_buffer.append(event)\n",
    "    \n",
    "    # Determine Activity Action (start, between, end) of the Event\n",
    "    activity_boundaries_classifier.classify_event(event)\n",
    "    activity_action = event.activity_action\n",
    "    \n",
    "    # Activity-Event Assignment\n",
    "    \n",
    "    # Start Events initialize a new stack\n",
    "    if activity_action == \"Activity Start\": \n",
    "        stacks.append(Stack(SETTING,event))\n",
    "        \n",
    "    # Between Events need to be matched to a current stack\n",
    "    if activity_action == \"NoAction\": \n",
    "        \n",
    "        # Only one stack open - no matching necessary\n",
    "        if len(stacks) == 1: \n",
    "            stacks[0].append_event(event)\n",
    "        \n",
    "        # event has a origin package number. Assign to the stack containing the origin event\n",
    "        elif event.origin_request_frame: \n",
    "            idx = search_stack_for_request_frame(event.origin_request_frame, stacks)\n",
    "            stacks[idx].append_event(event)\n",
    "        \n",
    "        else:             \n",
    "            # create exclude list that contains stack with different attributes than the event\n",
    "            exclude_indices =  event_activity_model.exclude_stacks_by_attribute(stacks, event)\n",
    "    \n",
    "            # check for potential match with event attributes\n",
    "            stack_index:int = event_activity_model.check_stack_attributes(stacks, event, exclude_indices)\n",
    "                   \n",
    "            # index = -1 indicates no match has been found yet \n",
    "            if stack_index == -1:    \n",
    "                \n",
    "                # assign to stack via pattern matching\n",
    "                stack_index = event_activity_model.assign_to_sequence(event,stacks, 4, exclude_indices)\n",
    "            \n",
    "            # fallback to stream index\n",
    "            if stack_index == -1: \n",
    "                stack_index = search_stream_index(stacks, event, exclude_indices)    \n",
    "            \n",
    "            # fallback: add to first confident stack not in exclude indices\n",
    "            if stack_index == -1:\n",
    "                res = next((i for i in range(len(stacks)) if i not in exclude_indices and stacks[i].confidence),-1)\n",
    "                stack_index = res\n",
    "                \n",
    "            # add event to determined index\n",
    "            stacks[stack_index].append_event(event)\n",
    "        \n",
    "    if activity_action == \"Activity End\":\n",
    "        # end events contain a request frame. Add to stack containing the request frame\n",
    "        stack_index = search_stack_for_request_frame(event.origin_request_frame, stacks)\n",
    "        stacks[stack_index].append_event(event)\n",
    "        \n",
    "        # only emit the stack if the end-event classification was confident\n",
    "        if event.confidence: \n",
    "            if len(stacks) > 1: \n",
    "                stack = stacks.pop(stack_index)\n",
    "                activity_type_classifier.classify_stack(stack)\n",
    "                stacks_out.append(stack)\n",
    "                \n",
    "            else: \n",
    "                event.confidence = False\n",
    "     \n",
    "\n",
    "    # Emit closed stacks and handle uncertain events \n",
    "    for idx, stack in enumerate(stacks):\n",
    "        last_event = stack[-1]\n",
    "        # check for uncertain \"NoAction\" (between) events. These could be end events.\n",
    "        if not last_event.confidence and last_event.activity_action == \"NoAction\":\n",
    "            # If no event has been added to the stack for N event loops, pop and emit it. \n",
    "            if event_loop_index - last_event.event_loop_index > EVENT_LOOP_CUTOFF_NO_ACTION: \n",
    "                stacks.pop(idx)\n",
    "                activity_type_classifier.classify_stack(stack)\n",
    "                stacks_out.append(stack)\n",
    "                \n",
    "    for idx, stack in enumerate(stacks): \n",
    "        last_event = stack.events[-1]\n",
    "        # check for uncertain \"End Events\". These could be between events.\n",
    "        if not last_event.confidence and last_event.activity_action == \"Activity End\": \n",
    "            # If no event has been added to the stack for N event loops, pop it and emit it.\n",
    "            if event_loop_index - last_event.event_loop_index > EVENT_LOOP_CUTOFF_END_EVENT: \n",
    "            \n",
    "                # we are now sure to pop the stack. \n",
    "                stacks.pop(idx)\n",
    "                activity_type_classifier.classify_stack(stack)\n",
    "                stacks_out.append(stack) \n",
    "                \n",
    "    end_time = time.time()\n",
    "    processing_times.append(end_time - start_time)\n",
    "                \n",
    "# pop all stacks that are still left at the end of the event loop\n",
    "for stack in stacks: \n",
    "    activity_type_classifier.classify_stack(stack)\n",
    "    stacks_out.append(stack)  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:33.129920Z",
     "start_time": "2024-05-07T08:37:32.674005Z"
    }
   },
   "id": "41016b5653419b1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b0cb44ba78a095"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "df_test, test_labels = create_eval_dataframe(df_gt, df_il_in)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:33.314666Z",
     "start_time": "2024-05-07T08:37:33.142467Z"
    }
   },
   "id": "af84593eb02114e4"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "stack_predictions = [stack.activity_type for stack in stacks_out]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:33.319963Z",
     "start_time": "2024-05-07T08:37:33.314836Z"
    }
   },
   "id": "6f1ad7aebd575fc8"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "    start  actual_end  start_pred  end_pred  end_pred_true  start_pred_true  \\\n0      17         325          17       325           True             True   \n1     356        1192         356      1192           True             True   \n2    1212        1520        1212      1520           True             True   \n3    1582        2336        1582      2336           True             True   \n4    2354        2664        2354      2664           True             True   \n5    2708        4461        2708      4461           True             True   \n6    3057        4871        3057      4871           True             True   \n7    4467        4881        4467      4881           True             True   \n8    4939        6164        4939      6164           True             True   \n9    5606        6859        5606      6859           True             True   \n10   6879        7190        6879      7190           True             True   \n11   7261        8044        7261      8044           True             True   \n12   8060        8374        8060      8374           True             True   \n13   8421        9329        8421      9329           True             True   \n14   9105       10237        9105     10237           True             True   \n15  10272       10786       10272     10786           True             True   \n16  10384       11067       10384     11067           True             True   \n17  11099       11941       11099     11941           True             True   \n18  11974       14049       11974     14049           True             True   \n19  12074       14533       12074     14533           True             True   \n20  13457       14328       13457     14328           True             True   \n21  14576       15350       14576     15350           True             True   \n22  15377       17235       15377     17235           True             True   \n23  15720       16615       15720     16615           True             True   \n24  16049       17037       16049     17037           True             True   \n25  17273       18876       17273     18876           True             True   \n26  17801       19551       17801     19551           True             True   \n27  19585       20665       19585     20665           True             True   \n28  20245       20557       20245     20557           True             True   \n29  20696       21362       20696     21362           True             True   \n30  20928       21651       20928     21651           True             True   \n31  21674       22664       21674     22664           True             True   \n32  22687       26505       22687     26505           True             True   \n33  22851       25173       22851     25173           True             True   \n34  23848       27222       23848     27222           True             True   \n35  25366       27235       25366     27235           True             True   \n36  27306       28384       27306     28384           True             True   \n\n    start_end_true  \n0             True  \n1             True  \n2             True  \n3             True  \n4             True  \n5             True  \n6             True  \n7             True  \n8             True  \n9             True  \n10            True  \n11            True  \n12            True  \n13            True  \n14            True  \n15            True  \n16            True  \n17            True  \n18            True  \n19            True  \n20            True  \n21            True  \n22            True  \n23            True  \n24            True  \n25            True  \n26            True  \n27            True  \n28            True  \n29            True  \n30            True  \n31            True  \n32            True  \n33            True  \n34            True  \n35            True  \n36            True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>start</th>\n      <th>actual_end</th>\n      <th>start_pred</th>\n      <th>end_pred</th>\n      <th>end_pred_true</th>\n      <th>start_pred_true</th>\n      <th>start_end_true</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17</td>\n      <td>325</td>\n      <td>17</td>\n      <td>325</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>356</td>\n      <td>1192</td>\n      <td>356</td>\n      <td>1192</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1212</td>\n      <td>1520</td>\n      <td>1212</td>\n      <td>1520</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1582</td>\n      <td>2336</td>\n      <td>1582</td>\n      <td>2336</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2354</td>\n      <td>2664</td>\n      <td>2354</td>\n      <td>2664</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2708</td>\n      <td>4461</td>\n      <td>2708</td>\n      <td>4461</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3057</td>\n      <td>4871</td>\n      <td>3057</td>\n      <td>4871</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>4467</td>\n      <td>4881</td>\n      <td>4467</td>\n      <td>4881</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4939</td>\n      <td>6164</td>\n      <td>4939</td>\n      <td>6164</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5606</td>\n      <td>6859</td>\n      <td>5606</td>\n      <td>6859</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>6879</td>\n      <td>7190</td>\n      <td>6879</td>\n      <td>7190</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7261</td>\n      <td>8044</td>\n      <td>7261</td>\n      <td>8044</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>8060</td>\n      <td>8374</td>\n      <td>8060</td>\n      <td>8374</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>8421</td>\n      <td>9329</td>\n      <td>8421</td>\n      <td>9329</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>9105</td>\n      <td>10237</td>\n      <td>9105</td>\n      <td>10237</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>10272</td>\n      <td>10786</td>\n      <td>10272</td>\n      <td>10786</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>10384</td>\n      <td>11067</td>\n      <td>10384</td>\n      <td>11067</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>11099</td>\n      <td>11941</td>\n      <td>11099</td>\n      <td>11941</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>11974</td>\n      <td>14049</td>\n      <td>11974</td>\n      <td>14049</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>12074</td>\n      <td>14533</td>\n      <td>12074</td>\n      <td>14533</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>13457</td>\n      <td>14328</td>\n      <td>13457</td>\n      <td>14328</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>14576</td>\n      <td>15350</td>\n      <td>14576</td>\n      <td>15350</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>15377</td>\n      <td>17235</td>\n      <td>15377</td>\n      <td>17235</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>15720</td>\n      <td>16615</td>\n      <td>15720</td>\n      <td>16615</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>16049</td>\n      <td>17037</td>\n      <td>16049</td>\n      <td>17037</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>17273</td>\n      <td>18876</td>\n      <td>17273</td>\n      <td>18876</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>17801</td>\n      <td>19551</td>\n      <td>17801</td>\n      <td>19551</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>19585</td>\n      <td>20665</td>\n      <td>19585</td>\n      <td>20665</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>20245</td>\n      <td>20557</td>\n      <td>20245</td>\n      <td>20557</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>20696</td>\n      <td>21362</td>\n      <td>20696</td>\n      <td>21362</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>20928</td>\n      <td>21651</td>\n      <td>20928</td>\n      <td>21651</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>21674</td>\n      <td>22664</td>\n      <td>21674</td>\n      <td>22664</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>22687</td>\n      <td>26505</td>\n      <td>22687</td>\n      <td>26505</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>22851</td>\n      <td>25173</td>\n      <td>22851</td>\n      <td>25173</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>23848</td>\n      <td>27222</td>\n      <td>23848</td>\n      <td>27222</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>25366</td>\n      <td>27235</td>\n      <td>25366</td>\n      <td>27235</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>27306</td>\n      <td>28384</td>\n      <td>27306</td>\n      <td>28384</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall matching accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "start = [stack[0].frame_number for stack in stacks_out]\n",
    "end = [stack[-1].frame_number for stack in stacks_out]\n",
    "\n",
    "res_df = pd.DataFrame({\"start_pred\":start, \"end_pred\":end})\n",
    "\n",
    "eval_df = df_gt[[\"start\", \"actual_end\"]].merge(res_df,how=\"left\", left_on =\"start\", right_on = \"start_pred\").fillna(-1).astype(int)\n",
    "eval_df[\"end_pred_true\"] = eval_df[\"actual_end\"] == eval_df[\"end_pred\"]\n",
    "eval_df[\"start_pred_true\"] = eval_df[\"start\"] == eval_df[\"start_pred\"]\n",
    "eval_df[\"start_end_true\"] =eval_df[\"start_pred_true\"] == eval_df[\"end_pred_true\"]\n",
    "\n",
    "display(eval_df)\n",
    "print(f\"Overall matching accuracy: {(eval_df['start_pred_true'].mean() + eval_df['end_pred_true'].mean()) / 2}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:33.330351Z",
     "start_time": "2024-05-07T08:37:33.321035Z"
    }
   },
   "id": "40a6a1f949ad0661"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "unique_no_nan = lambda x: list(filter(None, pd.unique(x)))\n",
    "first_unique = lambda x: unique_no_nan(x)[0]\n",
    "\n",
    "def compare_values(x,y):\n",
    "    # Multi index and casting magic - I just want to compare the bp_ids lol\n",
    "    x = int(x[0])\n",
    "    y = int(y[0])\n",
    "\n",
    "    return x == y\n",
    "\n",
    "if SETTING == \"PTP\": \n",
    "    # Create dataframe with mapping of frame numbers to event stacks\n",
    "    frame_numbers = [event.frame_number for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    stack_numbers = [idx for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    case_id = [stack.case_id[\"id\"]  if stack.case_id else -1 for idx, stack in enumerate(stacks_out) for event in stack]\n",
    "    sniff_time =  [event.sniff_time for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    sale_order_ids = [event.attributes[\"sale_order_id\"] for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    sale_order_line_ids = [event.attributes[\"sale_order_line_id\"] for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    purchase_requisition_ids = [event.attributes[\"purchase_requisition_id\"] for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    purchase_requisition_line_ids = [event.attributes[\"purchase_requisition_line_id\"] for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    purchase_order_ids = [event.attributes[\"purchase_order_id\"] for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    sale_order_line_id_case_id=  [stack.case_id[\"sale_order_line_id\"] if stack.case_id else -1 for idx, stack in enumerate(stacks_out) for event in stack]\n",
    "    \n",
    "    \n",
    "    df_frame_numbers = pd.DataFrame(data={\"frame.number\": frame_numbers, \"sniff_time\": sniff_time, \"stack_idx\": stack_numbers, \"sale_order_id\": sale_order_ids,\"sale_order_line_id\": sale_order_line_ids,\"sale_order_line_id_case_id\":sale_order_line_id_case_id,\"purchase_requisition_id\": purchase_requisition_ids,\"purchase_requisition_line_id\": purchase_requisition_line_ids, \"purchase_order_id\":purchase_order_ids, \"case_id\": case_id})\n",
    "    \n",
    "    # Merge Activity Name from ground truth frame to event sequences for evaluation\n",
    "    merged_df = df_frame_numbers.merge(df_gt[[\"activity_name\",\"start\",\"bp_id\"]], how=\"left\",left_on=\"frame.number\", right_on=\"start\").drop(columns=\"start\")\n",
    "    \n",
    "    merged_df[[\"activity_name\",\"bp_id\"]] = merged_df.groupby(\"stack_idx\")[[\"activity_name\",\"bp_id\"]].ffill()\n",
    "    \n",
    "    # Merge with filtered interleaved test data\n",
    "    merged_df = df_test.merge(merged_df, on=\"frame.number\")\n",
    "    \n",
    "    res = merged_df.groupby(\"stack_idx\").agg(sale_order_id = (\"sale_order_id\", unique_no_nan),sale_order_line_id=(\"sale_order_line_id\", unique_no_nan), sale_order_line_id_case_id=(\"sale_order_line_id_case_id\", unique_no_nan),purchase_requisition_id=(\"purchase_requisition_id\", unique_no_nan),purchase_requisition_line_id=(\"purchase_requisition_line_id\", unique_no_nan),purchase_order_id=(\"purchase_order_id\",unique_no_nan),case_id=(\"case_id\", first_unique),bp_id=(\"bp_id\", unique_no_nan),frame_number_min=(\"frame.number\",\"min\"),frame_number_max =  (\"frame.number\",\"max\"),sniff_time_min=(\"sniff_time_x\",\"min\"),sniff_time_max=(\"sniff_time_x\",\"min\"), activity_name=(\"activity_name\", lambda x: x.head(1)))\n",
    "    \n",
    "    res = res.merge(eval_df[[\"start_pred_true\",\"end_pred_true\",\"start_end_true\"]], left_index=True, right_index=True)\n",
    "    res[\"stack_prediction\"] = stack_predictions\n",
    "    # Apply the custom function to compare 'sale_order_line_id' and 'sale_order_line_id_case_id'\n",
    "    res[\"bp_true\"] = res.apply(lambda x: compare_values(x[\"sale_order_line_id_case_id\"], x[\"bp_id\"]), axis = 1)\n",
    "    res[\"activity_true\"] = res[\"activity_name\"] ==  res[\"stack_prediction\"]\n",
    "    #res.loc[\"Mean\",\"bp_true\"] = res[\"bp_true\"].mean()\n",
    "    #res.loc[\"Mean\",\"activity_true\"] = res[\"activity_true\"].mean()\n",
    "    \n",
    "elif SETTING == \"HR\": \n",
    "    # Create dataframe with mapping of frame numbers to event stacks\n",
    "    frame_numbers = [event.frame_number for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    stack_numbers = [idx for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    applicant_ids = [event.attributes[\"applicant_id\"] for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    activity_ids = [event.attributes[\"activity_id\"] for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    mail_ids = [event.attributes[\"mail_id\"] for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    sniff_time =  [event.sniff_time for idx,stack in enumerate(stacks_out) for event in stack]\n",
    "    case_id = [stack.case_id[\"id\"]  if stack.case_id else -1 for idx, stack in enumerate(stacks_out) for event in stack]\n",
    "    \n",
    "    df_frame_numbers = pd.DataFrame(data={\"frame.number\": frame_numbers, \"sniff_time\": sniff_time, \"stack_idx\": stack_numbers, \"applicant_id\": applicant_ids,\"activity_id\": activity_ids, \"mail_id\":mail_ids,\"case_id\": case_id})\n",
    "    \n",
    "    # Merge Activity Name from ground truth frame to event sequences for evaluation\n",
    "    merged_df = df_frame_numbers.merge(df_gt[[\"activity_name\",\"start\",\"bp_id\"]], how=\"left\",left_on=\"frame.number\", right_on=\"start\").drop(columns=\"start\")\n",
    "    \n",
    "    merged_df[[\"activity_name\",\"bp_id\"]] = merged_df.groupby(\"stack_idx\")[[\"activity_name\",\"bp_id\"]].ffill()\n",
    "    #merged_df[\"activity_name\"] = merged_df.groupby(\"stack_idx\")[\"bp_id\"].ffill()\n",
    "    \n",
    "    # Merge with filtered interleaved test data\n",
    "    merged_df = df_test.merge(merged_df, on=\"frame.number\")  \n",
    "    \n",
    "    res = merged_df.groupby(\"stack_idx\").agg(applicant_id = (\"applicant_id\", unique_no_nan),activity_id=(\"activity_id\", unique_no_nan), mail_id=(\"mail_id\", unique_no_nan),case_id=(\"case_id\", first_unique),bp_id=(\"bp_id\", unique_no_nan),frame_number_min=(\"frame.number\",\"min\"),frame_number_max =  (\"frame.number\",\"max\"),sniff_time_min=(\"sniff_time_x\",\"min\"),sniff_time_max=(\"sniff_time_x\",\"min\"), activity_name=(\"activity_name\", lambda x: x.head(1)))\n",
    "    res[\"stack_prediction\"] = stack_predictions\n",
    "    # Apply the custom function to compare 'sale_order_line_id' and 'sale_order_line_id_case_id'\n",
    "    res[\"bp_true\"] = res.apply(lambda x: compare_values(x[\"applicant_id\"], x[\"bp_id\"]), axis = 1)\n",
    "    res[\"activity_true\"] = res[\"activity_name\"] ==  res[\"stack_prediction\"]\n",
    "    #res.loc[\"Mean\",\"bp_true\"] = res[\"bp_true\"].mean()\n",
    "    #res.loc[\"Mean\",\"activity_true\"] = res[\"activity_true\"].mean()  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:33.396600Z",
     "start_time": "2024-05-07T08:37:33.344599Z"
    }
   },
   "id": "e2db163cbe61470d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Activity Boundaries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fb69f9220272eb0"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "  Activity End       1.00      1.00      1.00        37\n",
      "Activity Start       1.00      1.00      1.00        37\n",
      "      NoAction       1.00      1.00      1.00      1239\n",
      "\n",
      "      accuracy                           1.00      1313\n",
      "     macro avg       1.00      1.00      1.00      1313\n",
      "  weighted avg       1.00      1.00      1.00      1313\n"
     ]
    }
   ],
   "source": [
    "df_aa_test = pd.DataFrame(df_test[[\"frame.number\", \"ActivityAction\"]])\n",
    "df_aa_test[\"ActivityAction\"] = \"NoAction\"\n",
    "df_aa_test.loc[df_aa_test[\"frame.number\"].isin(eval_df[\"end_pred\"]), \"ActivityAction\"] = \"Activity End\"\n",
    "df_aa_test.loc[df_aa_test[\"frame.number\"].isin(eval_df[\"start_pred\"]), \"ActivityAction\"] = \"Activity Start\"\n",
    "print(classification_report(test_labels, df_aa_test[\"ActivityAction\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:33.401788Z",
     "start_time": "2024-05-07T08:37:33.362702Z"
    }
   },
   "id": "843ea84011358669"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Activity Type "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f2d62ab2512083a"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "               ContractProposal       1.00      1.00      1.00         1\n",
      " GenerateJobApplicationActivity       1.00      1.00      1.00        10\n",
      "         PerformAnInterviewCall       1.00      1.00      1.00         5\n",
      "      PerformAnInterviewMeeting       1.00      1.00      1.00         3\n",
      "           ResumeReviewActivity       1.00      1.00      1.00        10\n",
      "ScheduleAnInterviewActivityCall       1.00      1.00      1.00         5\n",
      "     ScheduleAnInterviewMeeting       1.00      1.00      1.00         3\n",
      "\n",
      "                       accuracy                           1.00        37\n",
      "                      macro avg       1.00      1.00      1.00        37\n",
      "                   weighted avg       1.00      1.00      1.00        37\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(res[\"activity_name\"], res[\"stack_prediction\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:33.403608Z",
     "start_time": "2024-05-07T08:37:33.383572Z"
    }
   },
   "id": "5b10d616efa1e470"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Case ID"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8f5014b03db7f80"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Activity Type --------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1224       1.00      1.00      1.00         2\n",
      "        1225       1.00      1.00      1.00         4\n",
      "        1226       1.00      1.00      1.00         2\n",
      "        1227       1.00      1.00      1.00         2\n",
      "        1228       1.00      1.00      1.00         6\n",
      "        1229       1.00      1.00      1.00         7\n",
      "        1230       1.00      1.00      1.00         2\n",
      "        1231       1.00      1.00      1.00         6\n",
      "        1232       1.00      1.00      1.00         4\n",
      "        1233       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00        37\n",
      "   macro avg       1.00      1.00      1.00        37\n",
      "weighted avg       1.00      1.00      1.00        37\n"
     ]
    }
   ],
   "source": [
    "first_int = lambda x: int(x[0])\n",
    "pred = res[case_id_primary].map(first_int)\n",
    "true = res[\"bp_id\"].map(first_int)\n",
    "\n",
    "print(\"------------------ Activity Type --------------\")\n",
    "print(classification_report(true[pred!= -1], pred[pred!= -1], zero_division=0.0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:33.403791Z",
     "start_time": "2024-05-07T08:37:33.390267Z"
    }
   },
   "id": "769244aaab09be9e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "events_out = res.sort_values(by= \"sniff_time_min\")[[\"sniff_time_min\",\"stack_prediction\",\"case_id\"]].reset_index(drop=True)\n",
    "events_out.columns = [\"timestamp\", \"activity\", \"case_id\"]\n",
    "events_out[\"timestamp\"] = pd.to_datetime(events_out[\"timestamp\"])\n",
    "events_out[\"case_id\"] = events_out[\"case_id\"].astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:33.405221Z",
     "start_time": "2024-05-07T08:37:33.400606Z"
    }
   },
   "id": "e1f87f8fa560c7e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conformance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73461e7e39c7b5e0"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import pm4py\n",
    "\n",
    "bpmn_graph = pm4py.read_bpmn(bpmn_path)\n",
    "petri_net, im, fm = pm4py.convert_to_petri_net(bpmn_graph)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:34.142762Z",
     "start_time": "2024-05-07T08:37:33.403211Z"
    }
   },
   "id": "3a84173abdbffdec"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "args = {\"case_id_key\":'case_id', \"activity_key\":'activity', \"timestamp_key\":'timestamp'}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:34.148392Z",
     "start_time": "2024-05-07T08:37:34.143384Z"
    }
   },
   "id": "d74df2ca73a68397"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "aligning log, completed variants ::   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbc7c9c0a98e4615858c8aad7b3cfef6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "computing precision with alignments, completed variants ::   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dae41a4713b47139d2801b1427f5166"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:   1.0\n",
      "Fitness:     0.9999649135118066\n",
      "_______________________________\n",
      "Conformance: 0.9999824567559032\n"
     ]
    }
   ],
   "source": [
    "log_fitness = pm4py.conformance.fitness_alignments(events_out, petri_net,im,fm, **args)[\"log_fitness\"]\n",
    "\n",
    "log_precision =pm4py.conformance.precision_alignments(events_out, petri_net,im,fm, **args)\n",
    "\n",
    "print(\"Precision:  \", log_precision)\n",
    "print(\"Fitness:    \",log_fitness )\n",
    "print(\"_______________________________\")\n",
    "print(\"Conformance:\", (log_fitness+log_precision) /2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:34.246094Z",
     "start_time": "2024-05-07T08:37:34.151124Z"
    }
   },
   "id": "941e28c7a5cabe23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Efficiency"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "521735032a1328a4"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model sizes in MB 0.7847\n",
      "average processing time: 0.3015 ms\n",
      "max buffer size: 154 (0.2508)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "model_size = [\n",
    "    sys.getsizeof(pickle.dumps(activity_boundaries_classifier.model)),\n",
    "    sys.getsizeof(pickle.dumps(activity_type_classifier.model)),\n",
    "    sys.getsizeof(pickle.dumps(event_activity_model.model))\n",
    "]\n",
    "print(\"model sizes in MB\", f\"{sum(model_size) / (1024**2):.4f}\")\n",
    "\n",
    "from statistics import mean,stdev\n",
    "print(\"average processing time:\",f\"{mean(processing_times)*1000:.4f} ms\")\n",
    "print(\"max buffer size:\", max(buffer_sizes),f\"({max(buffer_sizes) / len(records) * 100:.4f})\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:37:34.269653Z",
     "start_time": "2024-05-07T08:37:34.262322Z"
    }
   },
   "id": "efbbed3dfba849db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
