{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seb/git/master_thesis/nw_event_abstraction/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# surpress future warning related to SkLearn\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from keras.src.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.src.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from numpy import argmax, array\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.utils import compute_sample_weight\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tqdm.keras import TqdmCallback\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "from keras.layers import Bidirectional,Dense, Embedding, LSTM, SpatialDropout1D,BatchNormalization,GaussianNoise\n",
    "from keras import Input\n",
    "from keras.metrics import F1Score\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T13:57:36.771970Z",
     "start_time": "2023-12-18T13:57:32.864594Z"
    }
   },
   "id": "70db922f20c0fde0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df_r4 = pd.read_csv('../data_mod/R4_Train_poslabel.csv')\n",
    "df_r4_interleaved = pd.read_csv('../data_mod/R4_HR_Interleaved_poslabel.csv')\n",
    "\n",
    "df_r1 = pd.read_csv('../data_mod/R1_Train_poslabel.csv')\n",
    "df_r1_interleaved = pd.read_csv('../data_mod/R1_HR_Interleaved_poslabel.csv')\n",
    "\n",
    "df2_r4 = pd.read_csv('../data_mod/R4/R4_Valid.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T13:57:43.890001Z",
     "start_time": "2023-12-18T13:57:38.454569Z"
    }
   },
   "id": "985a5b9cc08c63a2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def sliding_window(observation, forward, backward): \n",
    "    window_size = forward+backward+1\n",
    "    sequences = sliding_window_view(observation, window_size)\n",
    "    \n",
    "    # pad first n-1 elements\n",
    "    #pad_top = [[0] * (sequence_length - 1 - i) + sequences[0, 0:i + 1].tolist() for i in range(sequence_length - 1)]\n",
    "    \n",
    "    #res = np.insert(sequences,0, pad_top, axis=0)\n",
    "    #res = np.asarray(X).astype(np.float32) \n",
    "    #res = res.reshape(res.shape[0], res.shape[1], 1)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def generate_sequences(observation,end_indices, start_indices, forward, backward, label_encoder):\n",
    "    # window size for stubs is 1 smaller than regular window size\n",
    "    window_size = forward+backward\n",
    "    # generate a sliding window over the observation with the new window size\n",
    "    seq = sliding_window_view(observation, window_size)\n",
    "    \n",
    "    # get all windows that end on \"position_end\" as indicated by end_indices\n",
    "    end_windows = [seq[i-window_size+1 ] for i in end_indices]\n",
    "    \n",
    "    # get all windows that START on \"position_start\" as indicated by start_indices\n",
    "    start_windows = [seq[i] for i in start_indices]\n",
    "    \n",
    "    # generate all combinations of ...end, start... \n",
    "    flattened_combinations = [np.concatenate(combination).tolist() for combination in product(end_windows, start_windows)]\n",
    "    \n",
    "    # generate new sequences by applying the sliding window over the new combinations\n",
    "    sequences = np.concatenate([sliding_window_view(c, window_size+1).tolist() for c in flattened_combinations])\n",
    "\n",
    "    \n",
    "    labels = label_encoder.transform([\"position_between\"] * (window_size-1) + [\"position_end\"] + [\"position_start\"] + [\"position_between\"] * (window_size-1))\n",
    "    labels = to_categorical(np.tile(labels[backward: -forward], len(flattened_combinations)))\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "def remove_duplicates(features, labels): \n",
    "    data = np.column_stack((features, labels))\n",
    "    unique_data = np.unique(data, axis = 0)\n",
    "    unique_train_features = unique_data[:,:-3]\n",
    "    unique_train_labels = unique_data[:,-3:]\n",
    "    \n",
    "    print(\"unique labels\", np.unique(unique_train_labels, axis = 0, return_counts=True))\n",
    "    \n",
    "    return unique_train_features, unique_train_labels\n",
    "\n",
    "def preprocess_data(train_features,train_labels, valid_features, valid_labels,forward_window, backward_window):\n",
    "    # encode message type for both training and validation data\n",
    "    message_type_encoder = LabelEncoder().fit(train_features.values)\n",
    "    train_features_encoded = message_type_encoder.transform(train_features) +1\n",
    "    valid_features_encoded = message_type_encoder.transform(valid_features) +1 \n",
    "\n",
    "    # generate feature sequences \n",
    "    \n",
    "    train_feature_sequence = sliding_window(train_features_encoded,forward_window, backward_window)\n",
    "    valid_feature_sequence = sliding_window(valid_features_encoded, forward_window, backward_window)\n",
    "    \n",
    "    \n",
    "    # generate more start / end overlaps \n",
    "    end_indices = train_labels[train_labels == \"position_end\"].index.tolist()\n",
    "    start_indices = train_labels[train_labels == \"position_start\"].index.tolist()\n",
    "\n",
    "    # one hot encode labels\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_encoded = to_categorical(label_encoder.transform(train_labels[backward_window: -forward_window]))\n",
    "    valid_labels_encoded = to_categorical(label_encoder.transform(valid_labels[backward_window: -forward_window]))\n",
    "    \n",
    "    unique_x, unique_y = remove_duplicates(train_feature_sequence, train_labels_encoded)\n",
    "\n",
    "    \n",
    "    synth_sequences, synth_labels = generate_sequences(train_features_encoded, end_indices, start_indices, forward_window, backward_window, label_encoder)\n",
    " \n",
    "    print(f\"generated {len(synth_sequences)} additional samples\")    \n",
    "    \n",
    "    train_feature_sequence = np.concatenate((train_feature_sequence, synth_sequences), axis = 0)\n",
    "    train_labels_encoded = np.concatenate((train_labels_encoded, synth_labels), axis = 0)\n",
    "        \n",
    "    # remove duplicates\n",
    "    features, labels = remove_duplicates(train_feature_sequence, train_labels_encoded)\n",
    "\n",
    "    \n",
    "    return features, labels, valid_feature_sequence, valid_labels_encoded, label_encoder.classes_.tolist(), len(message_type_encoder.classes_)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T14:00:04.052607Z",
     "start_time": "2023-12-18T14:00:04.043961Z"
    }
   },
   "id": "32691b68b7bf4c55"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique labels (array([[0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [1., 0., 0.]]), array([ 20,  19, 449]))\n",
      "generated 13057335 additional samples\n",
      "unique labels (array([[0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [1., 0., 0.]]), array([ 77,  70, 875]))\n",
      "1022\n",
      "1022\n"
     ]
    }
   ],
   "source": [
    "x, Y, x_il, Y_il, labels, label_length = preprocess_data(train_features = df_r4[\"event_with_roles\"], train_labels = df_r4[\"task_position\"],\n",
    "                                           valid_features = df_r4_interleaved[\"event_with_roles\"], valid_labels = df_r4_interleaved[\"task_position\"],\n",
    "                                           forward_window = 10, backward_window = 5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, Y, test_size = 0.3, shuffle = True, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T13:59:13.699271Z",
     "start_time": "2023-12-18T13:57:48.167658Z"
    }
   },
   "id": "de3f23d69d499642"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0., 1.]), array([2044, 1022]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y, return_counts=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T14:00:40.231477Z",
     "start_time": "2023-12-18T14:00:40.210406Z"
    }
   },
   "id": "6793bca244faccd8"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "        0       1\n0     0.0     1.0\n1  2044.0  1022.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2044.0</td>\n      <td>1022.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T14:00:50.020391Z",
     "start_time": "2023-12-18T14:00:49.999360Z"
    }
   },
   "id": "445869ff523f43c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ae5178871033f6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
